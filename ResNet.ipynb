{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10RandomLabels(datasets.CIFAR10):\n",
    "    \"\"\"CIFAR10 dataset, with support for randomly corrupt labels.\n",
    "    Params\n",
    "    ------  \n",
    "    corrupt_prob: float\n",
    "    Default 0.0. The probability of a label being replaced with\n",
    "    random label.\n",
    "    num_classes: int\n",
    "    Default 10. The number of classes in the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, corrupt_prob=0.0, num_classes=10, **kwargs):\n",
    "        super(CIFAR10RandomLabels, self).__init__(**kwargs)\n",
    "        self.n_classes = num_classes\n",
    "        if corrupt_prob > 0:\n",
    "            self.corrupt_labels(corrupt_prob)\n",
    "\n",
    "    def corrupt_labels(self, corrupt_prob):\n",
    "        labels = np.array(self.targets)\n",
    "        np.random.seed(12345)\n",
    "        mask = np.random.rand(len(labels)) <= corrupt_prob\n",
    "        rnd_labels = np.random.choice(self.n_classes, mask.sum())\n",
    "        labels[mask] = rnd_labels\n",
    "        # we need to explicitly cast the labels from npy.int64 to\n",
    "        # builtin int type, otherwise pytorch will fail...\n",
    "        labels = [int(x) for x in labels]\n",
    "        self.targets = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "        CIFAR10RandomLabels(root='./data', train=True, download=True, \n",
    "                            transform=transform, corrupt_prob=0.1),\n",
    "                            batch_size=128, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "        CIFAR10RandomLabels(root='./data', train=False, download=True,\n",
    "                            transform=transform, corrupt_prob=0.1),\n",
    "                            batch_size=100, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MaskedLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.mask_flag = False\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        self.mask = Variable(mask, requires_grad=False)\n",
    "        self.weight.data = self.weight.data*self.mask.data\n",
    "        self.mask_flag = True\n",
    "    \n",
    "    def get_mask(self):\n",
    "        print(self.mask_flag)\n",
    "        return self.mask\n",
    "    \n",
    "    def prune(self, threshold):\n",
    "        # generate mask\n",
    "        for params in self.parameters():\n",
    "            if len(params.data.size()) != 1: # Not bias\n",
    "                mask = params.data.abs() > threshold\n",
    "                self.set_mask(mask.float()) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.mask_flag == True:\n",
    "            weight = self.weight*self.mask\n",
    "            return F.linear(x, weight, self.bias)\n",
    "        else:\n",
    "            return F.linear(x, self.weight, self.bias)\n",
    "        \n",
    "\n",
    "        \n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(MaskedConv2d, self).__init__(in_channels, out_channels, \n",
    "            kernel_size, stride, padding, dilation, groups, bias)\n",
    "        self.mask_flag = False\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        self.mask = Variable(mask, requires_grad=False)\n",
    "        self.weight.data = self.weight.data*self.mask.data\n",
    "        self.mask_flag = True\n",
    "    \n",
    "    def get_mask(self):\n",
    "        print(self.mask_flag)\n",
    "        return self.mask\n",
    "    \n",
    "    def prune(self, threshold):\n",
    "        # generate mask\n",
    "        for params in self.parameters():\n",
    "            if len(params.data.size()) != 1: # Not bias\n",
    "                mask = params.data.abs() > threshold\n",
    "                self.set_mask(mask.float()) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.mask_flag == True:\n",
    "            weight = self.weight*self.mask\n",
    "            return F.conv2d(x, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "        else:\n",
    "            return F.conv2d(x, self.weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        conv2d = MaskedConv2d\n",
    "        self.conv1 = conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        conv2d = MaskedConv2d\n",
    "        self.conv1 = conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        linear = MaskedLinear\n",
    "        conv2d = MaskedConv2d\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "    def weight_prune(self, pruning_perc):\n",
    "        all_weights = []\n",
    "        for params in self.parameters():\n",
    "            if len(params.data.size()) != 1:\n",
    "                all_weights += list(params.cpu().data.abs().numpy().flatten())\n",
    "        threshold = np.percentile(np.array(all_weights), pruning_perc)\n",
    "        print(f'Pruning with threshold : %.4f' % threshold)\n",
    "        \n",
    "        # Module here refers to layer\n",
    "        prune_name = ['conv1','conv2','linear']\n",
    "        for name, module in self.named_modules():\n",
    "            if any(x in name for x in prune_name):\n",
    "                module.prune(threshold)   \n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "      MaskedConv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "      MaskedConv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "      MaskedConv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
      "      MaskedConv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "     MaskedConv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
      "     MaskedConv2d-13          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-14          [-1, 128, 16, 16]             256\n",
      "     MaskedConv2d-15          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
      "     MaskedConv2d-17          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-19          [-1, 128, 16, 16]               0\n",
      "     MaskedConv2d-20          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
      "     MaskedConv2d-22          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-24          [-1, 128, 16, 16]               0\n",
      "     MaskedConv2d-25            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-26            [-1, 256, 8, 8]             512\n",
      "     MaskedConv2d-27            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-28            [-1, 256, 8, 8]             512\n",
      "     MaskedConv2d-29            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-30            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-31            [-1, 256, 8, 8]               0\n",
      "     MaskedConv2d-32            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-33            [-1, 256, 8, 8]             512\n",
      "     MaskedConv2d-34            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-35            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-36            [-1, 256, 8, 8]               0\n",
      "     MaskedConv2d-37            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
      "     MaskedConv2d-39            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n",
      "     MaskedConv2d-41            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-43            [-1, 512, 4, 4]               0\n",
      "     MaskedConv2d-44            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
      "     MaskedConv2d-46            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-48            [-1, 512, 4, 4]               0\n",
      "     MaskedLinear-49                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 11.25\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 53.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "net.to(device)\n",
    "summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params[4] are weights in the first layer, params[5] are the masks, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv1\n",
      "bn1\n",
      "layer1\n",
      "layer1.0\n",
      "layer1.0.conv1\n",
      "layer1.0.bn1\n",
      "layer1.0.conv2\n",
      "layer1.0.bn2\n",
      "layer1.0.shortcut\n",
      "layer1.1\n",
      "layer1.1.conv1\n",
      "layer1.1.bn1\n",
      "layer1.1.conv2\n",
      "layer1.1.bn2\n",
      "layer1.1.shortcut\n",
      "layer2\n",
      "layer2.0\n",
      "layer2.0.conv1\n",
      "layer2.0.bn1\n",
      "layer2.0.conv2\n",
      "layer2.0.bn2\n",
      "layer2.0.shortcut\n",
      "layer2.0.shortcut.0\n",
      "layer2.0.shortcut.1\n",
      "layer2.1\n",
      "layer2.1.conv1\n",
      "layer2.1.bn1\n",
      "layer2.1.conv2\n",
      "layer2.1.bn2\n",
      "layer2.1.shortcut\n",
      "layer3\n",
      "layer3.0\n",
      "layer3.0.conv1\n",
      "layer3.0.bn1\n",
      "layer3.0.conv2\n",
      "layer3.0.bn2\n",
      "layer3.0.shortcut\n",
      "layer3.0.shortcut.0\n",
      "layer3.0.shortcut.1\n",
      "layer3.1\n",
      "layer3.1.conv1\n",
      "layer3.1.bn1\n",
      "layer3.1.conv2\n",
      "layer3.1.bn2\n",
      "layer3.1.shortcut\n",
      "layer4\n",
      "layer4.0\n",
      "layer4.0.conv1\n",
      "layer4.0.bn1\n",
      "layer4.0.conv2\n",
      "layer4.0.bn2\n",
      "layer4.0.shortcut\n",
      "layer4.0.shortcut.0\n",
      "layer4.0.shortcut.1\n",
      "layer4.1\n",
      "layer4.1.conv1\n",
      "layer4.1.bn1\n",
      "layer4.1.conv2\n",
      "layer4.1.bn2\n",
      "layer4.1.shortcut\n",
      "linear\n"
     ]
    }
   ],
   "source": [
    "#params = list(net.parameters())\n",
    "#for name, module in net.named_modules():\n",
    "#    print(name)\n",
    "#print(params[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Softmax is built in it so you do not need add that on the last layer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "initial_optimizer_state_dict = optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "            # training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        print('Train Accuracy: %.3f %%' % (100 * train_correct / train_total))\n",
    "        \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "    \n",
    "    print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n",
    "        100 * correct / total))\n",
    "    print('Test loss: %.4f ' % (test_loss / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_size(model):\n",
    "    non_zeros = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad==True:\n",
    "            non_zeros += param.nonzero().size(0)\n",
    "    params_size = non_zeros* 4. / (1024 ** 2)\n",
    "    print(\"Params size (MB): %0.3f\" % params_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 61.06 %\n",
      "Test loss: 1.2373 \n",
      "Params size (MB): 42.625\n"
     ]
    }
   ],
   "source": [
    "#train(net, 2)\n",
    "torch.save(net.state_dict(), \"models/ResNet.pt\")\n",
    "test(net)\n",
    "calculate_size(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with threshold : 0.0170\n",
      "Accuracy of the network on the 10000 test images: 11.70 %\n",
      "Test loss: 2.6761 \n",
      "Params size (MB): 4.460\n"
     ]
    }
   ],
   "source": [
    "net_p = ResNet18()\n",
    "net_p.load_state_dict(torch.load(\"models/ResNet.pt\"))\n",
    "net_p.to(device)\n",
    "net_p.eval()\n",
    "net_p.weight_prune(90)\n",
    "test(net_p)\n",
    "#summary(net, (3, 32, 32))\n",
    "calculate_size(net_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 1.943\n",
      "[1,   400] loss: 1.690\n",
      "[1,   600] loss: 1.570\n",
      "Train Accuracy: 41.732 %\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 48.20 %\n",
      "Test loss: 1.5034 \n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net_p.parameters(), lr=0.01, momentum=0.9)\n",
    "train(net_p, 1)\n",
    "test(net_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
