{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.modules.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PruningModule(Module):\n",
    "    def prune_by_percentile(self, q=5.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Note:\n",
    "             The pruning percentile is based on all layer's parameters concatenated\n",
    "        Args:\n",
    "            q (float): percentile in float\n",
    "            **kwargs: may contain `cuda`\n",
    "        \"\"\"\n",
    "        # Calculate percentile value\n",
    "        alive_parameters = []\n",
    "        for name, p in self.named_parameters():\n",
    "            # We do not prune bias term\n",
    "            if 'bias' in name or 'mask' in name:\n",
    "                continue\n",
    "            tensor = p.data.cpu().numpy()\n",
    "            alive = tensor[np.nonzero(tensor)] # flattened array of nonzero values\n",
    "            alive_parameters.append(alive)\n",
    "\n",
    "        all_alives = np.concatenate(alive_parameters)\n",
    "        percentile_value = np.percentile(abs(all_alives), q)\n",
    "        print(f'Pruning with threshold : {percentile_value}')\n",
    "\n",
    "        # Prune the weights and mask\n",
    "        # Note that module here is the layer\n",
    "        # ex) fc1, fc2, fc3\n",
    "        for name, module in self.named_modules():\n",
    "            if name in ['fc1', 'fc2','fc3']:\n",
    "                module.prune(threshold=percentile_value)\n",
    "\n",
    "    def prune_by_std(self, s=0.25):\n",
    "        \"\"\"\n",
    "        Note that `s` is a quality parameter / sensitivity value according to the paper.\n",
    "        According to Song Han's previous paper (Learning both Weights and Connections for Efficient Neural Networks),\n",
    "        'The pruning threshold is chosen as a quality parameter multiplied by the standard deviation of a layerâ€™s weights'\n",
    "\n",
    "        I tried multiple values and empirically, 0.25 matches the paper's compression rate and number of parameters.\n",
    "        Note : In the paper, the authors used different sensitivity values for different layers.\n",
    "        \"\"\"\n",
    "        for name, module in self.named_modules():\n",
    "            if name in ['fc1', 'fc2','fc3']:\n",
    "                threshold = np.std(module.weight.data.cpu().numpy()) * s\n",
    "                print(f'Pruning with threshold : {threshold} for layer {name}')\n",
    "                module.prune(threshold)\n",
    "\n",
    "\n",
    "class MaskedLinear(Module):\n",
    "    \"\"\"Applies a masked linear transformation to the incoming data: :math:`y = (A * M)x + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n",
    "          additional dimensions\n",
    "        - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
    "          are the same shape as the input.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            (out_features x in_features)\n",
    "        bias:   the learnable bias of the module of shape (out_features)\n",
    "        mask: the unlearnable mask for the weight.\n",
    "            It has the same shape as weight (out_features x in_features)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MaskedLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        # Initialize the mask with 1\n",
    "        self.mask = Parameter(torch.ones([out_features, in_features]), requires_grad=False)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight * self.mask, self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', bias=' + str(self.bias is not None) + ')'\n",
    "\n",
    "    def prune(self, threshold):\n",
    "        weight_dev = self.weight.device\n",
    "        mask_dev = self.mask.device\n",
    "        # Convert Tensors to numpy and calculate\n",
    "        tensor = self.weight.data.cpu().numpy()\n",
    "        mask = self.mask.data.cpu().numpy()\n",
    "        new_mask = np.where(abs(tensor) < threshold, 0.0, mask)\n",
    "        # Apply new weight and mask\n",
    "        self.weight.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
    "        self.mask.data = torch.from_numpy(new_mask).to(mask_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): MaskedLinear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): MaskedLinear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): MaskedLinear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(PruningModule):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        linear = MaskedLinear\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = linear(120, 84)\n",
    "        self.fc3 = linear(84, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "# Freeze the first layer\n",
    "#for param in net.fc1.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "# Initialize the first layer\n",
    "#def weights_init(m):\n",
    "#    if isinstance(m, nn.Linear):\n",
    "#        m.weight.data.normal_(0, 0.01)\n",
    "    \n",
    "#net.apply(weights_init)\n",
    "\n",
    "#net.prune_by_std()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params[4] are weights in the first layer, params[5] are the masks, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "Parameter containing:\n",
      "tensor([[-0.0289,  0.0409,  0.0139,  ...,  0.0412, -0.0190,  0.0167],\n",
      "        [ 0.0203, -0.0047, -0.0322,  ...,  0.0252, -0.0394, -0.0326],\n",
      "        [-0.0003, -0.0068, -0.0043,  ...,  0.0417, -0.0143,  0.0086],\n",
      "        ...,\n",
      "        [-0.0047,  0.0422,  0.0188,  ...,  0.0183,  0.0103, -0.0428],\n",
      "        [ 0.0191, -0.0187,  0.0427,  ...,  0.0431, -0.0050,  0.0374],\n",
      "        [-0.0170,  0.0247, -0.0037,  ...,  0.0441,  0.0049,  0.0115]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Softmax is built in it so you do not need add that on the last layer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 1000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 2.305\n",
      "[1,   400] loss: 2.303\n",
      "[1,   600] loss: 2.302\n",
      "[2,   200] loss: 2.300\n",
      "[2,   400] loss: 2.297\n",
      "[2,   600] loss: 2.292\n",
      "[3,   200] loss: 2.236\n",
      "[3,   400] loss: 2.185\n",
      "[3,   600] loss: 2.137\n",
      "[4,   200] loss: 2.059\n",
      "[4,   400] loss: 1.992\n",
      "[4,   600] loss: 1.910\n",
      "[5,   200] loss: 1.777\n",
      "[5,   400] loss: 1.734\n",
      "[5,   600] loss: 1.695\n",
      "[6,   200] loss: 1.638\n",
      "[6,   400] loss: 1.619\n",
      "[6,   600] loss: 1.602\n",
      "[7,   200] loss: 1.575\n",
      "[7,   400] loss: 1.538\n",
      "[7,   600] loss: 1.522\n",
      "[8,   200] loss: 1.493\n",
      "[8,   400] loss: 1.486\n",
      "[8,   600] loss: 1.488\n",
      "[9,   200] loss: 1.438\n",
      "[9,   400] loss: 1.440\n",
      "[9,   600] loss: 1.436\n",
      "[10,   200] loss: 1.418\n",
      "[10,   400] loss: 1.399\n",
      "[10,   600] loss: 1.386\n",
      "[11,   200] loss: 1.377\n",
      "[11,   400] loss: 1.360\n",
      "[11,   600] loss: 1.350\n",
      "[12,   200] loss: 1.340\n",
      "[12,   400] loss: 1.331\n",
      "[12,   600] loss: 1.304\n",
      "[13,   200] loss: 1.298\n",
      "[13,   400] loss: 1.294\n",
      "[13,   600] loss: 1.284\n",
      "[14,   200] loss: 1.260\n",
      "[14,   400] loss: 1.252\n",
      "[14,   600] loss: 1.254\n",
      "[15,   200] loss: 1.215\n",
      "[15,   400] loss: 1.227\n",
      "[15,   600] loss: 1.223\n",
      "[16,   200] loss: 1.198\n",
      "[16,   400] loss: 1.191\n",
      "[16,   600] loss: 1.172\n",
      "[17,   200] loss: 1.160\n",
      "[17,   400] loss: 1.148\n",
      "[17,   600] loss: 1.154\n",
      "[18,   200] loss: 1.139\n",
      "[18,   400] loss: 1.140\n",
      "[18,   600] loss: 1.130\n",
      "[19,   200] loss: 1.106\n",
      "[19,   400] loss: 1.101\n",
      "[19,   600] loss: 1.102\n",
      "[20,   200] loss: 1.075\n",
      "[20,   400] loss: 1.086\n",
      "[20,   600] loss: 1.079\n",
      "[21,   200] loss: 1.064\n",
      "[21,   400] loss: 1.064\n",
      "[21,   600] loss: 1.045\n",
      "[22,   200] loss: 1.048\n",
      "[22,   400] loss: 1.023\n",
      "[22,   600] loss: 1.032\n",
      "[23,   200] loss: 1.020\n",
      "[23,   400] loss: 1.002\n",
      "[23,   600] loss: 1.018\n",
      "[24,   200] loss: 1.001\n",
      "[24,   400] loss: 1.003\n",
      "[24,   600] loss: 0.982\n",
      "[25,   200] loss: 0.981\n",
      "[25,   400] loss: 0.986\n",
      "[25,   600] loss: 0.976\n",
      "[26,   200] loss: 0.955\n",
      "[26,   400] loss: 0.938\n",
      "[26,   600] loss: 0.963\n",
      "[27,   200] loss: 0.940\n",
      "[27,   400] loss: 0.936\n",
      "[27,   600] loss: 0.959\n",
      "[28,   200] loss: 0.922\n",
      "[28,   400] loss: 0.939\n",
      "[28,   600] loss: 0.922\n",
      "[29,   200] loss: 0.906\n",
      "[29,   400] loss: 0.910\n",
      "[29,   600] loss: 0.927\n",
      "[30,   200] loss: 0.898\n",
      "[30,   400] loss: 0.892\n",
      "[30,   600] loss: 0.890\n",
      "[31,   200] loss: 0.871\n",
      "[31,   400] loss: 0.874\n",
      "[31,   600] loss: 0.903\n",
      "[32,   200] loss: 0.843\n",
      "[32,   400] loss: 0.863\n",
      "[32,   600] loss: 0.882\n",
      "[33,   200] loss: 0.856\n",
      "[33,   400] loss: 0.852\n",
      "[33,   600] loss: 0.855\n",
      "[34,   200] loss: 0.827\n",
      "[34,   400] loss: 0.848\n",
      "[34,   600] loss: 0.834\n",
      "[35,   200] loss: 0.837\n",
      "[35,   400] loss: 0.835\n",
      "[35,   600] loss: 0.830\n",
      "[36,   200] loss: 0.810\n",
      "[36,   400] loss: 0.812\n",
      "[36,   600] loss: 0.816\n",
      "[37,   200] loss: 0.804\n",
      "[37,   400] loss: 0.798\n",
      "[37,   600] loss: 0.803\n",
      "[38,   200] loss: 0.784\n",
      "[38,   400] loss: 0.785\n",
      "[38,   600] loss: 0.808\n",
      "[39,   200] loss: 0.776\n",
      "[39,   400] loss: 0.784\n",
      "[39,   600] loss: 0.789\n",
      "[40,   200] loss: 0.759\n",
      "[40,   400] loss: 0.771\n",
      "[40,   600] loss: 0.792\n",
      "[41,   200] loss: 0.749\n",
      "[41,   400] loss: 0.758\n",
      "[41,   600] loss: 0.780\n",
      "[42,   200] loss: 0.740\n",
      "[42,   400] loss: 0.755\n",
      "[42,   600] loss: 0.754\n",
      "[43,   200] loss: 0.723\n",
      "[43,   400] loss: 0.749\n",
      "[43,   600] loss: 0.731\n",
      "[44,   200] loss: 0.716\n",
      "[44,   400] loss: 0.720\n",
      "[44,   600] loss: 0.734\n",
      "[45,   200] loss: 0.697\n",
      "[45,   400] loss: 0.720\n",
      "[45,   600] loss: 0.721\n",
      "[46,   200] loss: 0.690\n",
      "[46,   400] loss: 0.718\n",
      "[46,   600] loss: 0.706\n",
      "[47,   200] loss: 0.686\n",
      "[47,   400] loss: 0.679\n",
      "[47,   600] loss: 0.716\n",
      "[48,   200] loss: 0.677\n",
      "[48,   400] loss: 0.682\n",
      "[48,   600] loss: 0.693\n",
      "[49,   200] loss: 0.658\n",
      "[49,   400] loss: 0.665\n",
      "[49,   600] loss: 0.684\n",
      "[50,   200] loss: 0.646\n",
      "[50,   400] loss: 0.645\n",
      "[50,   600] loss: 0.689\n",
      "[51,   200] loss: 0.654\n",
      "[51,   400] loss: 0.650\n",
      "[51,   600] loss: 0.648\n",
      "[52,   200] loss: 0.624\n",
      "[52,   400] loss: 0.640\n",
      "[52,   600] loss: 0.653\n",
      "[53,   200] loss: 0.623\n",
      "[53,   400] loss: 0.644\n",
      "[53,   600] loss: 0.636\n",
      "[54,   200] loss: 0.606\n",
      "[54,   400] loss: 0.629\n",
      "[54,   600] loss: 0.636\n",
      "[55,   200] loss: 0.600\n",
      "[55,   400] loss: 0.614\n",
      "[55,   600] loss: 0.630\n",
      "[56,   200] loss: 0.595\n",
      "[56,   400] loss: 0.595\n",
      "[56,   600] loss: 0.621\n",
      "[57,   200] loss: 0.564\n",
      "[57,   400] loss: 0.607\n",
      "[57,   600] loss: 0.612\n",
      "[58,   200] loss: 0.566\n",
      "[58,   400] loss: 0.589\n",
      "[58,   600] loss: 0.615\n",
      "[59,   200] loss: 0.557\n",
      "[59,   400] loss: 0.584\n",
      "[59,   600] loss: 0.584\n",
      "[60,   200] loss: 0.542\n",
      "[60,   400] loss: 0.571\n",
      "[60,   600] loss: 0.583\n",
      "[61,   200] loss: 0.548\n",
      "[61,   400] loss: 0.556\n",
      "[61,   600] loss: 0.565\n",
      "[62,   200] loss: 0.520\n",
      "[62,   400] loss: 0.552\n",
      "[62,   600] loss: 0.568\n",
      "[63,   200] loss: 0.528\n",
      "[63,   400] loss: 0.547\n",
      "[63,   600] loss: 0.549\n",
      "[64,   200] loss: 0.514\n",
      "[64,   400] loss: 0.540\n",
      "[64,   600] loss: 0.549\n",
      "[65,   200] loss: 0.498\n",
      "[65,   400] loss: 0.537\n",
      "[65,   600] loss: 0.529\n",
      "[66,   200] loss: 0.506\n",
      "[66,   400] loss: 0.507\n",
      "[66,   600] loss: 0.531\n",
      "[67,   200] loss: 0.496\n",
      "[67,   400] loss: 0.508\n",
      "[67,   600] loss: 0.526\n",
      "[68,   200] loss: 0.484\n",
      "[68,   400] loss: 0.507\n",
      "[68,   600] loss: 0.498\n",
      "[69,   200] loss: 0.474\n",
      "[69,   400] loss: 0.477\n",
      "[69,   600] loss: 0.519\n",
      "[70,   200] loss: 0.480\n",
      "[70,   400] loss: 0.479\n",
      "[70,   600] loss: 0.500\n",
      "[71,   200] loss: 0.456\n",
      "[71,   400] loss: 0.470\n",
      "[71,   600] loss: 0.485\n",
      "[72,   200] loss: 0.448\n",
      "[72,   400] loss: 0.452\n",
      "[72,   600] loss: 0.479\n",
      "[73,   200] loss: 0.430\n",
      "[73,   400] loss: 0.470\n",
      "[73,   600] loss: 0.477\n",
      "[74,   200] loss: 0.426\n",
      "[74,   400] loss: 0.460\n",
      "[74,   600] loss: 0.450\n",
      "[75,   200] loss: 0.412\n",
      "[75,   400] loss: 0.448\n",
      "[75,   600] loss: 0.465\n",
      "[76,   200] loss: 0.404\n",
      "[76,   400] loss: 0.422\n",
      "[76,   600] loss: 0.448\n",
      "[77,   200] loss: 0.410\n",
      "[77,   400] loss: 0.418\n",
      "[77,   600] loss: 0.428\n",
      "[78,   200] loss: 0.394\n",
      "[78,   400] loss: 0.415\n",
      "[78,   600] loss: 0.427\n",
      "[79,   200] loss: 0.398\n",
      "[79,   400] loss: 0.400\n",
      "[79,   600] loss: 0.418\n",
      "[80,   200] loss: 0.375\n",
      "[80,   400] loss: 0.405\n",
      "[80,   600] loss: 0.417\n",
      "[81,   200] loss: 0.382\n",
      "[81,   400] loss: 0.396\n",
      "[81,   600] loss: 0.413\n",
      "[82,   200] loss: 0.365\n",
      "[82,   400] loss: 0.390\n",
      "[82,   600] loss: 0.386\n",
      "[83,   200] loss: 0.361\n",
      "[83,   400] loss: 0.374\n",
      "[83,   600] loss: 0.400\n",
      "[84,   200] loss: 0.351\n",
      "[84,   400] loss: 0.380\n",
      "[84,   600] loss: 0.375\n",
      "[85,   200] loss: 0.334\n",
      "[85,   400] loss: 0.357\n",
      "[85,   600] loss: 0.380\n",
      "[86,   200] loss: 0.332\n",
      "[86,   400] loss: 0.355\n",
      "[86,   600] loss: 0.378\n",
      "[87,   200] loss: 0.320\n",
      "[87,   400] loss: 0.351\n",
      "[87,   600] loss: 0.367\n",
      "[88,   200] loss: 0.338\n",
      "[88,   400] loss: 0.338\n",
      "[88,   600] loss: 0.376\n",
      "[89,   200] loss: 0.308\n",
      "[89,   400] loss: 0.343\n",
      "[89,   600] loss: 0.352\n",
      "[90,   200] loss: 0.313\n",
      "[90,   400] loss: 0.333\n",
      "[90,   600] loss: 0.337\n",
      "[91,   200] loss: 0.290\n",
      "[91,   400] loss: 0.313\n",
      "[91,   600] loss: 0.344\n",
      "[92,   200] loss: 0.291\n",
      "[92,   400] loss: 0.311\n",
      "[92,   600] loss: 0.322\n",
      "[93,   200] loss: 0.283\n",
      "[93,   400] loss: 0.301\n",
      "[93,   600] loss: 0.332\n",
      "[94,   200] loss: 0.282\n",
      "[94,   400] loss: 0.298\n",
      "[94,   600] loss: 0.330\n",
      "[95,   200] loss: 0.272\n",
      "[95,   400] loss: 0.305\n",
      "[95,   600] loss: 0.306\n",
      "[96,   200] loss: 0.262\n",
      "[96,   400] loss: 0.278\n",
      "[96,   600] loss: 0.311\n",
      "[97,   200] loss: 0.268\n",
      "[97,   400] loss: 0.276\n",
      "[97,   600] loss: 0.310\n",
      "[98,   200] loss: 0.259\n",
      "[98,   400] loss: 0.276\n",
      "[98,   600] loss: 0.303\n",
      "[99,   200] loss: 0.246\n",
      "[99,   400] loss: 0.278\n",
      "[99,   600] loss: 0.293\n",
      "[100,   200] loss: 0.250\n",
      "[100,   400] loss: 0.271\n",
      "[100,   600] loss: 0.285\n",
      "[101,   200] loss: 0.238\n",
      "[101,   400] loss: 0.263\n",
      "[101,   600] loss: 0.292\n",
      "[102,   200] loss: 0.234\n",
      "[102,   400] loss: 0.255\n",
      "[102,   600] loss: 0.280\n",
      "[103,   200] loss: 0.230\n",
      "[103,   400] loss: 0.271\n",
      "[103,   600] loss: 0.261\n",
      "[104,   200] loss: 0.226\n",
      "[104,   400] loss: 0.229\n",
      "[104,   600] loss: 0.272\n",
      "[105,   200] loss: 0.231\n",
      "[105,   400] loss: 0.233\n",
      "[105,   600] loss: 0.272\n",
      "[106,   200] loss: 0.225\n",
      "[106,   400] loss: 0.237\n",
      "[106,   600] loss: 0.258\n",
      "[107,   200] loss: 0.211\n",
      "[107,   400] loss: 0.225\n",
      "[107,   600] loss: 0.256\n",
      "[108,   200] loss: 0.208\n",
      "[108,   400] loss: 0.238\n",
      "[108,   600] loss: 0.271\n",
      "[109,   200] loss: 0.195\n",
      "[109,   400] loss: 0.227\n",
      "[109,   600] loss: 0.256\n",
      "[110,   200] loss: 0.199\n",
      "[110,   400] loss: 0.222\n",
      "[110,   600] loss: 0.230\n",
      "[111,   200] loss: 0.205\n",
      "[111,   400] loss: 0.213\n",
      "[111,   600] loss: 0.227\n",
      "[112,   200] loss: 0.190\n",
      "[112,   400] loss: 0.197\n",
      "[112,   600] loss: 0.238\n",
      "[113,   200] loss: 0.190\n",
      "[113,   400] loss: 0.207\n",
      "[113,   600] loss: 0.227\n",
      "[114,   200] loss: 0.186\n",
      "[114,   400] loss: 0.212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114,   600] loss: 0.236\n",
      "[115,   200] loss: 0.192\n",
      "[115,   400] loss: 0.194\n",
      "[115,   600] loss: 0.212\n",
      "[116,   200] loss: 0.157\n",
      "[116,   400] loss: 0.185\n",
      "[116,   600] loss: 0.188\n",
      "[117,   200] loss: 0.167\n",
      "[117,   400] loss: 0.196\n",
      "[117,   600] loss: 0.212\n",
      "[118,   200] loss: 0.170\n",
      "[118,   400] loss: 0.166\n",
      "[118,   600] loss: 0.202\n",
      "[119,   200] loss: 0.158\n",
      "[119,   400] loss: 0.183\n",
      "[119,   600] loss: 0.202\n",
      "[120,   200] loss: 0.168\n",
      "[120,   400] loss: 0.182\n",
      "[120,   600] loss: 0.201\n",
      "[121,   200] loss: 0.158\n",
      "[121,   400] loss: 0.172\n",
      "[121,   600] loss: 0.199\n",
      "[122,   200] loss: 0.164\n",
      "[122,   400] loss: 0.169\n",
      "[122,   600] loss: 0.190\n",
      "[123,   200] loss: 0.165\n",
      "[123,   400] loss: 0.164\n",
      "[123,   600] loss: 0.185\n",
      "[124,   200] loss: 0.148\n",
      "[124,   400] loss: 0.188\n",
      "[124,   600] loss: 0.207\n",
      "[125,   200] loss: 0.172\n",
      "[125,   400] loss: 0.193\n",
      "[125,   600] loss: 0.207\n",
      "[126,   200] loss: 0.162\n",
      "[126,   400] loss: 0.162\n",
      "[126,   600] loss: 0.168\n",
      "[127,   200] loss: 0.138\n",
      "[127,   400] loss: 0.150\n",
      "[127,   600] loss: 0.182\n",
      "[128,   200] loss: 0.132\n",
      "[128,   400] loss: 0.137\n",
      "[128,   600] loss: 0.164\n",
      "[129,   200] loss: 0.166\n",
      "[129,   400] loss: 0.165\n",
      "[129,   600] loss: 0.172\n",
      "[130,   200] loss: 0.160\n",
      "[130,   400] loss: 0.136\n",
      "[130,   600] loss: 0.155\n",
      "[131,   200] loss: 0.123\n",
      "[131,   400] loss: 0.122\n",
      "[131,   600] loss: 0.184\n",
      "[132,   200] loss: 0.136\n",
      "[132,   400] loss: 0.142\n",
      "[132,   600] loss: 0.156\n",
      "[133,   200] loss: 0.128\n",
      "[133,   400] loss: 0.158\n",
      "[133,   600] loss: 0.159\n",
      "[134,   200] loss: 0.146\n",
      "[134,   400] loss: 0.126\n",
      "[134,   600] loss: 0.162\n",
      "[135,   200] loss: 0.132\n",
      "[135,   400] loss: 0.150\n",
      "[135,   600] loss: 0.167\n",
      "[136,   200] loss: 0.134\n",
      "[136,   400] loss: 0.131\n",
      "[136,   600] loss: 0.162\n",
      "[137,   200] loss: 0.128\n",
      "[137,   400] loss: 0.131\n",
      "[137,   600] loss: 0.133\n",
      "[138,   200] loss: 0.121\n",
      "[138,   400] loss: 0.137\n",
      "[138,   600] loss: 0.159\n",
      "[139,   200] loss: 0.143\n",
      "[139,   400] loss: 0.118\n",
      "[139,   600] loss: 0.148\n",
      "[140,   200] loss: 0.112\n",
      "[140,   400] loss: 0.117\n",
      "[140,   600] loss: 0.135\n",
      "[141,   200] loss: 0.109\n",
      "[141,   400] loss: 0.127\n",
      "[141,   600] loss: 0.155\n",
      "[142,   200] loss: 0.115\n",
      "[142,   400] loss: 0.130\n",
      "[142,   600] loss: 0.146\n",
      "[143,   200] loss: 0.122\n",
      "[143,   400] loss: 0.126\n",
      "[143,   600] loss: 0.134\n",
      "[144,   200] loss: 0.109\n",
      "[144,   400] loss: 0.127\n",
      "[144,   600] loss: 0.137\n",
      "[145,   200] loss: 0.133\n",
      "[145,   400] loss: 0.122\n",
      "[145,   600] loss: 0.117\n",
      "[146,   200] loss: 0.154\n",
      "[146,   400] loss: 0.133\n",
      "[146,   600] loss: 0.133\n",
      "[147,   200] loss: 0.121\n",
      "[147,   400] loss: 0.106\n",
      "[147,   600] loss: 0.112\n",
      "[148,   200] loss: 0.100\n",
      "[148,   400] loss: 0.092\n",
      "[148,   600] loss: 0.102\n",
      "[149,   200] loss: 0.160\n",
      "[149,   400] loss: 0.116\n",
      "[149,   600] loss: 0.144\n",
      "[150,   200] loss: 0.113\n",
      "[150,   400] loss: 0.129\n",
      "[150,   600] loss: 0.130\n",
      "[151,   200] loss: 0.106\n",
      "[151,   400] loss: 0.094\n",
      "[151,   600] loss: 0.105\n",
      "[152,   200] loss: 0.072\n",
      "[152,   400] loss: 0.067\n",
      "[152,   600] loss: 0.088\n",
      "[153,   200] loss: 0.083\n",
      "[153,   400] loss: 0.106\n",
      "[153,   600] loss: 0.104\n",
      "[154,   200] loss: 0.106\n",
      "[154,   400] loss: 0.110\n",
      "[154,   600] loss: 0.178\n",
      "[155,   200] loss: 0.144\n",
      "[155,   400] loss: 0.126\n",
      "[155,   600] loss: 0.155\n",
      "[156,   200] loss: 0.084\n",
      "[156,   400] loss: 0.073\n",
      "[156,   600] loss: 0.103\n",
      "[157,   200] loss: 0.060\n",
      "[157,   400] loss: 0.065\n",
      "[157,   600] loss: 0.077\n",
      "[158,   200] loss: 0.054\n",
      "[158,   400] loss: 0.059\n",
      "[158,   600] loss: 0.073\n",
      "[159,   200] loss: 0.052\n",
      "[159,   400] loss: 0.048\n",
      "[159,   600] loss: 0.061\n",
      "[160,   200] loss: 0.067\n",
      "[160,   400] loss: 0.065\n",
      "[160,   600] loss: 0.058\n",
      "[161,   200] loss: 0.083\n",
      "[161,   400] loss: 0.070\n",
      "[161,   600] loss: 0.141\n",
      "[162,   200] loss: 0.111\n",
      "[162,   400] loss: 0.156\n",
      "[162,   600] loss: 0.183\n",
      "[163,   200] loss: 0.182\n",
      "[163,   400] loss: 0.154\n",
      "[163,   600] loss: 0.180\n",
      "[164,   200] loss: 0.181\n",
      "[164,   400] loss: 0.130\n",
      "[164,   600] loss: 0.143\n",
      "[165,   200] loss: 0.116\n",
      "[165,   400] loss: 0.102\n",
      "[165,   600] loss: 0.102\n",
      "[166,   200] loss: 0.090\n",
      "[166,   400] loss: 0.077\n",
      "[166,   600] loss: 0.093\n",
      "[167,   200] loss: 0.074\n",
      "[167,   400] loss: 0.084\n",
      "[167,   600] loss: 0.085\n",
      "[168,   200] loss: 0.069\n",
      "[168,   400] loss: 0.070\n",
      "[168,   600] loss: 0.065\n",
      "[169,   200] loss: 0.041\n",
      "[169,   400] loss: 0.037\n",
      "[169,   600] loss: 0.038\n",
      "[170,   200] loss: 0.043\n",
      "[170,   400] loss: 0.031\n",
      "[170,   600] loss: 0.042\n",
      "[171,   200] loss: 0.036\n",
      "[171,   400] loss: 0.029\n",
      "[171,   600] loss: 0.044\n",
      "[172,   200] loss: 0.099\n",
      "[172,   400] loss: 0.082\n",
      "[172,   600] loss: 0.105\n",
      "[173,   200] loss: 0.141\n",
      "[173,   400] loss: 0.147\n",
      "[173,   600] loss: 0.159\n",
      "[174,   200] loss: 0.136\n",
      "[174,   400] loss: 0.170\n",
      "[174,   600] loss: 0.179\n",
      "[175,   200] loss: 0.153\n",
      "[175,   400] loss: 0.137\n",
      "[175,   600] loss: 0.151\n",
      "[176,   200] loss: 0.113\n",
      "[176,   400] loss: 0.162\n",
      "[176,   600] loss: 0.137\n",
      "[177,   200] loss: 0.116\n",
      "[177,   400] loss: 0.083\n",
      "[177,   600] loss: 0.096\n",
      "[178,   200] loss: 0.069\n",
      "[178,   400] loss: 0.095\n",
      "[178,   600] loss: 0.080\n",
      "[179,   200] loss: 0.055\n",
      "[179,   400] loss: 0.072\n",
      "[179,   600] loss: 0.087\n",
      "[180,   200] loss: 0.054\n",
      "[180,   400] loss: 0.040\n",
      "[180,   600] loss: 0.047\n",
      "[181,   200] loss: 0.041\n",
      "[181,   400] loss: 0.032\n",
      "[181,   600] loss: 0.035\n",
      "[182,   200] loss: 0.035\n",
      "[182,   400] loss: 0.023\n",
      "[182,   600] loss: 0.021\n",
      "[183,   200] loss: 0.011\n",
      "[183,   400] loss: 0.009\n",
      "[183,   600] loss: 0.009\n",
      "[184,   200] loss: 0.006\n",
      "[184,   400] loss: 0.006\n",
      "[184,   600] loss: 0.006\n",
      "[185,   200] loss: 0.004\n",
      "[185,   400] loss: 0.005\n",
      "[185,   600] loss: 0.005\n",
      "[186,   200] loss: 0.004\n",
      "[186,   400] loss: 0.004\n",
      "[186,   600] loss: 0.004\n",
      "[187,   200] loss: 0.003\n",
      "[187,   400] loss: 0.003\n",
      "[187,   600] loss: 0.004\n",
      "[188,   200] loss: 0.003\n",
      "[188,   400] loss: 0.003\n",
      "[188,   600] loss: 0.003\n",
      "[189,   200] loss: 0.003\n",
      "[189,   400] loss: 0.003\n",
      "[189,   600] loss: 0.003\n",
      "[190,   200] loss: 0.003\n",
      "[190,   400] loss: 0.003\n",
      "[190,   600] loss: 0.003\n",
      "[191,   200] loss: 0.002\n",
      "[191,   400] loss: 0.003\n",
      "[191,   600] loss: 0.003\n",
      "[192,   200] loss: 0.002\n",
      "[192,   400] loss: 0.002\n",
      "[192,   600] loss: 0.002\n",
      "[193,   200] loss: 0.002\n",
      "[193,   400] loss: 0.002\n",
      "[193,   600] loss: 0.002\n",
      "[194,   200] loss: 0.002\n",
      "[194,   400] loss: 0.002\n",
      "[194,   600] loss: 0.002\n",
      "[195,   200] loss: 0.002\n",
      "[195,   400] loss: 0.002\n",
      "[195,   600] loss: 0.002\n",
      "[196,   200] loss: 0.002\n",
      "[196,   400] loss: 0.002\n",
      "[196,   600] loss: 0.002\n",
      "[197,   200] loss: 0.002\n",
      "[197,   400] loss: 0.002\n",
      "[197,   600] loss: 0.002\n",
      "[198,   200] loss: 0.002\n",
      "[198,   400] loss: 0.002\n",
      "[198,   600] loss: 0.002\n",
      "[199,   200] loss: 0.002\n",
      "[199,   400] loss: 0.002\n",
      "[199,   600] loss: 0.002\n",
      "[200,   200] loss: 0.002\n",
      "[200,   400] loss: 0.002\n",
      "[200,   600] loss: 0.002\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 62 %\n"
     ]
    }
   ],
   "source": [
    "train()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with threshold : 0.025127505883574486 for layer fc1\n",
      "Pruning with threshold : 0.03324936330318451 for layer fc2\n",
      "Pruning with threshold : 0.0784255862236023 for layer fc3\n",
      "Accuracy of the network on the 10000 test images: 61 %\n"
     ]
    }
   ],
   "source": [
    "print('parameters before pruning:')\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)\n",
    "net.prune_by_std()\n",
    "print('parameters after pruning:')\n",
    "#print(params[4])\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.071\n",
      "[1,   400] loss: 0.105\n",
      "[1,   600] loss: 0.118\n",
      "[2,   200] loss: 0.117\n",
      "[2,   400] loss: 0.118\n",
      "[2,   600] loss: 0.120\n",
      "[3,   200] loss: 0.066\n",
      "[3,   400] loss: 0.072\n",
      "[3,   600] loss: 0.094\n",
      "[4,   200] loss: 0.085\n",
      "[4,   400] loss: 0.089\n",
      "[4,   600] loss: 0.086\n",
      "[5,   200] loss: 0.055\n",
      "[5,   400] loss: 0.059\n",
      "[5,   600] loss: 0.047\n",
      "[6,   200] loss: 0.020\n",
      "[6,   400] loss: 0.017\n",
      "[6,   600] loss: 0.022\n",
      "[7,   200] loss: 0.012\n",
      "[7,   400] loss: 0.008\n",
      "[7,   600] loss: 0.007\n",
      "[8,   200] loss: 0.004\n",
      "[8,   400] loss: 0.004\n",
      "[8,   600] loss: 0.004\n",
      "[9,   200] loss: 0.003\n",
      "[9,   400] loss: 0.003\n",
      "[9,   600] loss: 0.003\n",
      "[10,   200] loss: 0.003\n",
      "[10,   400] loss: 0.003\n",
      "[10,   600] loss: 0.003\n",
      "[11,   200] loss: 0.002\n",
      "[11,   400] loss: 0.003\n",
      "[11,   600] loss: 0.003\n",
      "[12,   200] loss: 0.002\n",
      "[12,   400] loss: 0.002\n",
      "[12,   600] loss: 0.002\n",
      "[13,   200] loss: 0.002\n",
      "[13,   400] loss: 0.002\n",
      "[13,   600] loss: 0.002\n",
      "[14,   200] loss: 0.002\n",
      "[14,   400] loss: 0.002\n",
      "[14,   600] loss: 0.002\n",
      "[15,   200] loss: 0.002\n",
      "[15,   400] loss: 0.002\n",
      "[15,   600] loss: 0.002\n",
      "[16,   200] loss: 0.002\n",
      "[16,   400] loss: 0.002\n",
      "[16,   600] loss: 0.002\n",
      "[17,   200] loss: 0.002\n",
      "[17,   400] loss: 0.002\n",
      "[17,   600] loss: 0.002\n",
      "[18,   200] loss: 0.002\n",
      "[18,   400] loss: 0.002\n",
      "[18,   600] loss: 0.002\n",
      "[19,   200] loss: 0.002\n",
      "[19,   400] loss: 0.002\n",
      "[19,   600] loss: 0.002\n",
      "[20,   200] loss: 0.002\n",
      "[20,   400] loss: 0.002\n",
      "[20,   600] loss: 0.002\n",
      "[21,   200] loss: 0.001\n",
      "[21,   400] loss: 0.001\n",
      "[21,   600] loss: 0.002\n",
      "[22,   200] loss: 0.001\n",
      "[22,   400] loss: 0.001\n",
      "[22,   600] loss: 0.002\n",
      "[23,   200] loss: 0.001\n",
      "[23,   400] loss: 0.001\n",
      "[23,   600] loss: 0.001\n",
      "[24,   200] loss: 0.001\n",
      "[24,   400] loss: 0.001\n",
      "[24,   600] loss: 0.001\n",
      "[25,   200] loss: 0.001\n",
      "[25,   400] loss: 0.001\n",
      "[25,   600] loss: 0.001\n",
      "[26,   200] loss: 0.001\n",
      "[26,   400] loss: 0.001\n",
      "[26,   600] loss: 0.001\n",
      "[27,   200] loss: 0.001\n",
      "[27,   400] loss: 0.001\n",
      "[27,   600] loss: 0.001\n",
      "[28,   200] loss: 0.001\n",
      "[28,   400] loss: 0.001\n",
      "[28,   600] loss: 0.001\n",
      "[29,   200] loss: 0.001\n",
      "[29,   400] loss: 0.001\n",
      "[29,   600] loss: 0.001\n",
      "[30,   200] loss: 0.001\n",
      "[30,   400] loss: 0.001\n",
      "[30,   600] loss: 0.001\n",
      "[31,   200] loss: 0.001\n",
      "[31,   400] loss: 0.001\n",
      "[31,   600] loss: 0.001\n",
      "[32,   200] loss: 0.001\n",
      "[32,   400] loss: 0.001\n",
      "[32,   600] loss: 0.001\n",
      "[33,   200] loss: 0.001\n",
      "[33,   400] loss: 0.001\n",
      "[33,   600] loss: 0.001\n",
      "[34,   200] loss: 0.001\n",
      "[34,   400] loss: 0.001\n",
      "[34,   600] loss: 0.001\n",
      "[35,   200] loss: 0.001\n",
      "[35,   400] loss: 0.001\n",
      "[35,   600] loss: 0.001\n",
      "[36,   200] loss: 0.001\n",
      "[36,   400] loss: 0.001\n",
      "[36,   600] loss: 0.001\n",
      "[37,   200] loss: 0.001\n",
      "[37,   400] loss: 0.001\n",
      "[37,   600] loss: 0.001\n",
      "[38,   200] loss: 0.001\n",
      "[38,   400] loss: 0.001\n",
      "[38,   600] loss: 0.001\n",
      "[39,   200] loss: 0.001\n",
      "[39,   400] loss: 0.001\n",
      "[39,   600] loss: 0.001\n",
      "[40,   200] loss: 0.001\n",
      "[40,   400] loss: 0.001\n",
      "[40,   600] loss: 0.001\n",
      "[41,   200] loss: 0.001\n",
      "[41,   400] loss: 0.001\n",
      "[41,   600] loss: 0.001\n",
      "[42,   200] loss: 0.001\n",
      "[42,   400] loss: 0.001\n",
      "[42,   600] loss: 0.001\n",
      "[43,   200] loss: 0.001\n",
      "[43,   400] loss: 0.001\n",
      "[43,   600] loss: 0.001\n",
      "[44,   200] loss: 0.001\n",
      "[44,   400] loss: 0.001\n",
      "[44,   600] loss: 0.001\n",
      "[45,   200] loss: 0.001\n",
      "[45,   400] loss: 0.001\n",
      "[45,   600] loss: 0.001\n",
      "[46,   200] loss: 0.001\n",
      "[46,   400] loss: 0.001\n",
      "[46,   600] loss: 0.001\n",
      "[47,   200] loss: 0.001\n",
      "[47,   400] loss: 0.001\n",
      "[47,   600] loss: 0.001\n",
      "[48,   200] loss: 0.001\n",
      "[48,   400] loss: 0.001\n",
      "[48,   600] loss: 0.001\n",
      "[49,   200] loss: 0.001\n",
      "[49,   400] loss: 0.001\n",
      "[49,   600] loss: 0.001\n",
      "[50,   200] loss: 0.001\n",
      "[50,   400] loss: 0.001\n",
      "[50,   600] loss: 0.001\n",
      "[51,   200] loss: 0.001\n",
      "[51,   400] loss: 0.001\n",
      "[51,   600] loss: 0.001\n",
      "[52,   200] loss: 0.001\n",
      "[52,   400] loss: 0.001\n",
      "[52,   600] loss: 0.001\n",
      "[53,   200] loss: 0.001\n",
      "[53,   400] loss: 0.001\n",
      "[53,   600] loss: 0.001\n",
      "[54,   200] loss: 0.001\n",
      "[54,   400] loss: 0.001\n",
      "[54,   600] loss: 0.001\n",
      "[55,   200] loss: 0.001\n",
      "[55,   400] loss: 0.001\n",
      "[55,   600] loss: 0.001\n",
      "[56,   200] loss: 0.001\n",
      "[56,   400] loss: 0.001\n",
      "[56,   600] loss: 0.001\n",
      "[57,   200] loss: 0.001\n",
      "[57,   400] loss: 0.001\n",
      "[57,   600] loss: 0.001\n",
      "[58,   200] loss: 0.001\n",
      "[58,   400] loss: 0.001\n",
      "[58,   600] loss: 0.001\n",
      "[59,   200] loss: 0.001\n",
      "[59,   400] loss: 0.001\n",
      "[59,   600] loss: 0.001\n",
      "[60,   200] loss: 0.001\n",
      "[60,   400] loss: 0.001\n",
      "[60,   600] loss: 0.001\n",
      "[61,   200] loss: 0.001\n",
      "[61,   400] loss: 0.001\n",
      "[61,   600] loss: 0.001\n",
      "[62,   200] loss: 0.001\n",
      "[62,   400] loss: 0.001\n",
      "[62,   600] loss: 0.001\n",
      "[63,   200] loss: 0.001\n",
      "[63,   400] loss: 0.001\n",
      "[63,   600] loss: 0.001\n",
      "[64,   200] loss: 0.001\n",
      "[64,   400] loss: 0.001\n",
      "[64,   600] loss: 0.001\n",
      "[65,   200] loss: 0.001\n",
      "[65,   400] loss: 0.001\n",
      "[65,   600] loss: 0.001\n",
      "[66,   200] loss: 0.001\n",
      "[66,   400] loss: 0.001\n",
      "[66,   600] loss: 0.001\n",
      "[67,   200] loss: 0.001\n",
      "[67,   400] loss: 0.001\n",
      "[67,   600] loss: 0.001\n",
      "[68,   200] loss: 0.001\n",
      "[68,   400] loss: 0.001\n",
      "[68,   600] loss: 0.001\n",
      "[69,   200] loss: 0.001\n",
      "[69,   400] loss: 0.001\n",
      "[69,   600] loss: 0.001\n",
      "[70,   200] loss: 0.001\n",
      "[70,   400] loss: 0.001\n",
      "[70,   600] loss: 0.001\n",
      "[71,   200] loss: 0.001\n",
      "[71,   400] loss: 0.001\n",
      "[71,   600] loss: 0.001\n",
      "[72,   200] loss: 0.001\n",
      "[72,   400] loss: 0.001\n",
      "[72,   600] loss: 0.001\n",
      "[73,   200] loss: 0.001\n",
      "[73,   400] loss: 0.001\n",
      "[73,   600] loss: 0.001\n",
      "[74,   200] loss: 0.001\n",
      "[74,   400] loss: 0.001\n",
      "[74,   600] loss: 0.001\n",
      "[75,   200] loss: 0.001\n",
      "[75,   400] loss: 0.001\n",
      "[75,   600] loss: 0.001\n",
      "[76,   200] loss: 0.001\n",
      "[76,   400] loss: 0.001\n",
      "[76,   600] loss: 0.001\n",
      "[77,   200] loss: 0.001\n",
      "[77,   400] loss: 0.001\n",
      "[77,   600] loss: 0.001\n",
      "[78,   200] loss: 0.001\n",
      "[78,   400] loss: 0.001\n",
      "[78,   600] loss: 0.001\n",
      "[79,   200] loss: 0.001\n",
      "[79,   400] loss: 0.001\n",
      "[79,   600] loss: 0.001\n",
      "[80,   200] loss: 0.001\n",
      "[80,   400] loss: 0.001\n",
      "[80,   600] loss: 0.001\n",
      "[81,   200] loss: 0.001\n",
      "[81,   400] loss: 0.001\n",
      "[81,   600] loss: 0.001\n",
      "[82,   200] loss: 0.001\n",
      "[82,   400] loss: 0.001\n",
      "[82,   600] loss: 0.001\n",
      "[83,   200] loss: 0.001\n",
      "[83,   400] loss: 0.001\n",
      "[83,   600] loss: 0.001\n",
      "[84,   200] loss: 0.001\n",
      "[84,   400] loss: 0.001\n",
      "[84,   600] loss: 0.001\n",
      "[85,   200] loss: 0.000\n",
      "[85,   400] loss: 0.001\n",
      "[85,   600] loss: 0.001\n",
      "[86,   200] loss: 0.000\n",
      "[86,   400] loss: 0.001\n",
      "[86,   600] loss: 0.001\n",
      "[87,   200] loss: 0.001\n",
      "[87,   400] loss: 0.001\n",
      "[87,   600] loss: 0.001\n",
      "[88,   200] loss: 0.001\n",
      "[88,   400] loss: 0.000\n",
      "[88,   600] loss: 0.000\n",
      "[89,   200] loss: 0.000\n",
      "[89,   400] loss: 0.001\n",
      "[89,   600] loss: 0.000\n",
      "[90,   200] loss: 0.000\n",
      "[90,   400] loss: 0.001\n",
      "[90,   600] loss: 0.001\n",
      "[91,   200] loss: 0.000\n",
      "[91,   400] loss: 0.000\n",
      "[91,   600] loss: 0.000\n",
      "[92,   200] loss: 0.000\n",
      "[92,   400] loss: 0.000\n",
      "[92,   600] loss: 0.000\n",
      "[93,   200] loss: 0.000\n",
      "[93,   400] loss: 0.000\n",
      "[93,   600] loss: 0.000\n",
      "[94,   200] loss: 0.000\n",
      "[94,   400] loss: 0.000\n",
      "[94,   600] loss: 0.000\n",
      "[95,   200] loss: 0.000\n",
      "[95,   400] loss: 0.000\n",
      "[95,   600] loss: 0.000\n",
      "[96,   200] loss: 0.000\n",
      "[96,   400] loss: 0.000\n",
      "[96,   600] loss: 0.000\n",
      "[97,   200] loss: 0.000\n",
      "[97,   400] loss: 0.000\n",
      "[97,   600] loss: 0.000\n",
      "[98,   200] loss: 0.000\n",
      "[98,   400] loss: 0.000\n",
      "[98,   600] loss: 0.000\n",
      "[99,   200] loss: 0.000\n",
      "[99,   400] loss: 0.000\n",
      "[99,   600] loss: 0.000\n",
      "[100,   200] loss: 0.000\n",
      "[100,   400] loss: 0.000\n",
      "[100,   600] loss: 0.000\n",
      "[101,   200] loss: 0.000\n",
      "[101,   400] loss: 0.000\n",
      "[101,   600] loss: 0.000\n",
      "[102,   200] loss: 0.000\n",
      "[102,   400] loss: 0.000\n",
      "[102,   600] loss: 0.000\n",
      "[103,   200] loss: 0.000\n",
      "[103,   400] loss: 0.000\n",
      "[103,   600] loss: 0.000\n",
      "[104,   200] loss: 0.000\n",
      "[104,   400] loss: 0.000\n",
      "[104,   600] loss: 0.000\n",
      "[105,   200] loss: 0.000\n",
      "[105,   400] loss: 0.000\n",
      "[105,   600] loss: 0.000\n",
      "[106,   200] loss: 0.000\n",
      "[106,   400] loss: 0.000\n",
      "[106,   600] loss: 0.000\n",
      "[107,   200] loss: 0.000\n",
      "[107,   400] loss: 0.000\n",
      "[107,   600] loss: 0.000\n",
      "[108,   200] loss: 0.000\n",
      "[108,   400] loss: 0.000\n",
      "[108,   600] loss: 0.000\n",
      "[109,   200] loss: 0.000\n",
      "[109,   400] loss: 0.000\n",
      "[109,   600] loss: 0.000\n",
      "[110,   200] loss: 0.000\n",
      "[110,   400] loss: 0.000\n",
      "[110,   600] loss: 0.000\n",
      "[111,   200] loss: 0.000\n",
      "[111,   400] loss: 0.000\n",
      "[111,   600] loss: 0.000\n",
      "[112,   200] loss: 0.000\n",
      "[112,   400] loss: 0.000\n",
      "[112,   600] loss: 0.000\n",
      "[113,   200] loss: 0.000\n",
      "[113,   400] loss: 0.000\n",
      "[113,   600] loss: 0.000\n",
      "[114,   200] loss: 0.000\n",
      "[114,   400] loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114,   600] loss: 0.000\n",
      "[115,   200] loss: 0.000\n",
      "[115,   400] loss: 0.000\n",
      "[115,   600] loss: 0.000\n",
      "[116,   200] loss: 0.000\n",
      "[116,   400] loss: 0.000\n",
      "[116,   600] loss: 0.000\n",
      "[117,   200] loss: 0.000\n",
      "[117,   400] loss: 0.000\n",
      "[117,   600] loss: 0.000\n",
      "[118,   200] loss: 0.000\n",
      "[118,   400] loss: 0.000\n",
      "[118,   600] loss: 0.000\n",
      "[119,   200] loss: 0.000\n",
      "[119,   400] loss: 0.000\n",
      "[119,   600] loss: 0.000\n",
      "[120,   200] loss: 0.000\n",
      "[120,   400] loss: 0.000\n",
      "[120,   600] loss: 0.000\n",
      "[121,   200] loss: 0.000\n",
      "[121,   400] loss: 0.000\n",
      "[121,   600] loss: 0.000\n",
      "[122,   200] loss: 0.000\n",
      "[122,   400] loss: 0.000\n",
      "[122,   600] loss: 0.000\n",
      "[123,   200] loss: 0.000\n",
      "[123,   400] loss: 0.000\n",
      "[123,   600] loss: 0.000\n",
      "[124,   200] loss: 0.000\n",
      "[124,   400] loss: 0.000\n",
      "[124,   600] loss: 0.000\n",
      "[125,   200] loss: 0.000\n",
      "[125,   400] loss: 0.000\n",
      "[125,   600] loss: 0.000\n",
      "[126,   200] loss: 0.000\n",
      "[126,   400] loss: 0.000\n",
      "[126,   600] loss: 0.000\n",
      "[127,   200] loss: 0.000\n",
      "[127,   400] loss: 0.000\n",
      "[127,   600] loss: 0.000\n",
      "[128,   200] loss: 0.000\n",
      "[128,   400] loss: 0.000\n",
      "[128,   600] loss: 0.000\n",
      "[129,   200] loss: 0.000\n",
      "[129,   400] loss: 0.000\n",
      "[129,   600] loss: 0.000\n",
      "[130,   200] loss: 0.000\n",
      "[130,   400] loss: 0.000\n",
      "[130,   600] loss: 0.000\n",
      "[131,   200] loss: 0.000\n",
      "[131,   400] loss: 0.000\n",
      "[131,   600] loss: 0.000\n",
      "[132,   200] loss: 0.000\n",
      "[132,   400] loss: 0.000\n",
      "[132,   600] loss: 0.000\n",
      "[133,   200] loss: 0.000\n",
      "[133,   400] loss: 0.000\n",
      "[133,   600] loss: 0.000\n",
      "[134,   200] loss: 0.000\n",
      "[134,   400] loss: 0.000\n",
      "[134,   600] loss: 0.000\n",
      "[135,   200] loss: 0.000\n",
      "[135,   400] loss: 0.000\n",
      "[135,   600] loss: 0.000\n",
      "[136,   200] loss: 0.000\n",
      "[136,   400] loss: 0.000\n",
      "[136,   600] loss: 0.000\n",
      "[137,   200] loss: 0.000\n",
      "[137,   400] loss: 0.000\n",
      "[137,   600] loss: 0.000\n",
      "[138,   200] loss: 0.000\n",
      "[138,   400] loss: 0.000\n",
      "[138,   600] loss: 0.000\n",
      "[139,   200] loss: 0.000\n",
      "[139,   400] loss: 0.000\n",
      "[139,   600] loss: 0.000\n",
      "[140,   200] loss: 0.000\n",
      "[140,   400] loss: 0.000\n",
      "[140,   600] loss: 0.000\n",
      "[141,   200] loss: 0.000\n",
      "[141,   400] loss: 0.000\n",
      "[141,   600] loss: 0.000\n",
      "[142,   200] loss: 0.000\n",
      "[142,   400] loss: 0.000\n",
      "[142,   600] loss: 0.000\n",
      "[143,   200] loss: 0.000\n",
      "[143,   400] loss: 0.000\n",
      "[143,   600] loss: 0.000\n",
      "[144,   200] loss: 0.000\n",
      "[144,   400] loss: 0.000\n",
      "[144,   600] loss: 0.000\n",
      "[145,   200] loss: 0.000\n",
      "[145,   400] loss: 0.000\n",
      "[145,   600] loss: 0.000\n",
      "[146,   200] loss: 0.000\n",
      "[146,   400] loss: 0.000\n",
      "[146,   600] loss: 0.000\n",
      "[147,   200] loss: 0.000\n",
      "[147,   400] loss: 0.000\n",
      "[147,   600] loss: 0.000\n",
      "[148,   200] loss: 0.000\n",
      "[148,   400] loss: 0.000\n",
      "[148,   600] loss: 0.000\n",
      "[149,   200] loss: 0.000\n",
      "[149,   400] loss: 0.000\n",
      "[149,   600] loss: 0.000\n",
      "[150,   200] loss: 0.000\n",
      "[150,   400] loss: 0.000\n",
      "[150,   600] loss: 0.000\n",
      "[151,   200] loss: 0.000\n",
      "[151,   400] loss: 0.000\n",
      "[151,   600] loss: 0.000\n",
      "[152,   200] loss: 0.000\n",
      "[152,   400] loss: 0.000\n",
      "[152,   600] loss: 0.000\n",
      "[153,   200] loss: 0.000\n",
      "[153,   400] loss: 0.000\n",
      "[153,   600] loss: 0.000\n",
      "[154,   200] loss: 0.000\n",
      "[154,   400] loss: 0.000\n",
      "[154,   600] loss: 0.000\n",
      "[155,   200] loss: 0.000\n",
      "[155,   400] loss: 0.000\n",
      "[155,   600] loss: 0.000\n",
      "[156,   200] loss: 0.000\n",
      "[156,   400] loss: 0.000\n",
      "[156,   600] loss: 0.000\n",
      "[157,   200] loss: 0.000\n",
      "[157,   400] loss: 0.000\n",
      "[157,   600] loss: 0.000\n",
      "[158,   200] loss: 0.000\n",
      "[158,   400] loss: 0.000\n",
      "[158,   600] loss: 0.000\n",
      "[159,   200] loss: 0.000\n",
      "[159,   400] loss: 0.000\n",
      "[159,   600] loss: 0.000\n",
      "[160,   200] loss: 0.000\n",
      "[160,   400] loss: 0.000\n",
      "[160,   600] loss: 0.000\n",
      "[161,   200] loss: 0.000\n",
      "[161,   400] loss: 0.000\n",
      "[161,   600] loss: 0.000\n",
      "[162,   200] loss: 0.000\n",
      "[162,   400] loss: 0.000\n",
      "[162,   600] loss: 0.000\n",
      "[163,   200] loss: 0.000\n",
      "[163,   400] loss: 0.000\n",
      "[163,   600] loss: 0.000\n",
      "[164,   200] loss: 0.000\n",
      "[164,   400] loss: 0.000\n",
      "[164,   600] loss: 0.000\n",
      "[165,   200] loss: 0.000\n",
      "[165,   400] loss: 0.000\n",
      "[165,   600] loss: 0.000\n",
      "[166,   200] loss: 0.000\n",
      "[166,   400] loss: 0.000\n",
      "[166,   600] loss: 0.000\n",
      "[167,   200] loss: 0.000\n",
      "[167,   400] loss: 0.000\n",
      "[167,   600] loss: 0.000\n",
      "[168,   200] loss: 0.000\n",
      "[168,   400] loss: 0.000\n",
      "[168,   600] loss: 0.000\n",
      "[169,   200] loss: 0.000\n",
      "[169,   400] loss: 0.000\n",
      "[169,   600] loss: 0.000\n",
      "[170,   200] loss: 0.000\n",
      "[170,   400] loss: 0.000\n",
      "[170,   600] loss: 0.000\n",
      "[171,   200] loss: 0.000\n",
      "[171,   400] loss: 0.000\n",
      "[171,   600] loss: 0.000\n",
      "[172,   200] loss: 0.000\n",
      "[172,   400] loss: 0.000\n",
      "[172,   600] loss: 0.000\n",
      "[173,   200] loss: 0.000\n",
      "[173,   400] loss: 0.000\n",
      "[173,   600] loss: 0.000\n",
      "[174,   200] loss: 0.000\n",
      "[174,   400] loss: 0.000\n",
      "[174,   600] loss: 0.000\n",
      "[175,   200] loss: 0.000\n",
      "[175,   400] loss: 0.000\n",
      "[175,   600] loss: 0.000\n",
      "[176,   200] loss: 0.000\n",
      "[176,   400] loss: 0.000\n",
      "[176,   600] loss: 0.000\n",
      "[177,   200] loss: 0.000\n",
      "[177,   400] loss: 0.000\n",
      "[177,   600] loss: 0.000\n",
      "[178,   200] loss: 0.000\n",
      "[178,   400] loss: 0.000\n",
      "[178,   600] loss: 0.000\n",
      "[179,   200] loss: 0.000\n",
      "[179,   400] loss: 0.000\n",
      "[179,   600] loss: 0.000\n",
      "[180,   200] loss: 0.000\n",
      "[180,   400] loss: 0.000\n",
      "[180,   600] loss: 0.000\n",
      "[181,   200] loss: 0.000\n",
      "[181,   400] loss: 0.000\n",
      "[181,   600] loss: 0.000\n",
      "[182,   200] loss: 0.000\n",
      "[182,   400] loss: 0.000\n",
      "[182,   600] loss: 0.000\n",
      "[183,   200] loss: 0.000\n",
      "[183,   400] loss: 0.000\n",
      "[183,   600] loss: 0.000\n",
      "[184,   200] loss: 0.000\n",
      "[184,   400] loss: 0.000\n",
      "[184,   600] loss: 0.000\n",
      "[185,   200] loss: 0.000\n",
      "[185,   400] loss: 0.000\n",
      "[185,   600] loss: 0.000\n",
      "[186,   200] loss: 0.000\n",
      "[186,   400] loss: 0.000\n",
      "[186,   600] loss: 0.000\n",
      "[187,   200] loss: 0.000\n",
      "[187,   400] loss: 0.000\n",
      "[187,   600] loss: 0.000\n",
      "[188,   200] loss: 0.000\n",
      "[188,   400] loss: 0.000\n",
      "[188,   600] loss: 0.000\n",
      "[189,   200] loss: 0.000\n",
      "[189,   400] loss: 0.000\n",
      "[189,   600] loss: 0.000\n",
      "[190,   200] loss: 0.000\n",
      "[190,   400] loss: 0.000\n",
      "[190,   600] loss: 0.000\n",
      "[191,   200] loss: 0.000\n",
      "[191,   400] loss: 0.000\n",
      "[191,   600] loss: 0.000\n",
      "[192,   200] loss: 0.000\n",
      "[192,   400] loss: 0.000\n",
      "[192,   600] loss: 0.000\n",
      "[193,   200] loss: 0.000\n",
      "[193,   400] loss: 0.000\n",
      "[193,   600] loss: 0.000\n",
      "[194,   200] loss: 0.000\n",
      "[194,   400] loss: 0.000\n",
      "[194,   600] loss: 0.000\n",
      "[195,   200] loss: 0.000\n",
      "[195,   400] loss: 0.000\n",
      "[195,   600] loss: 0.000\n",
      "[196,   200] loss: 0.000\n",
      "[196,   400] loss: 0.000\n",
      "[196,   600] loss: 0.000\n",
      "[197,   200] loss: 0.000\n",
      "[197,   400] loss: 0.000\n",
      "[197,   600] loss: 0.000\n",
      "[198,   200] loss: 0.000\n",
      "[198,   400] loss: 0.000\n",
      "[198,   600] loss: 0.000\n",
      "[199,   200] loss: 0.000\n",
      "[199,   400] loss: 0.000\n",
      "[199,   600] loss: 0.000\n",
      "[200,   200] loss: 0.000\n",
      "[200,   400] loss: 0.000\n",
      "[200,   600] loss: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train()\n",
    "#print(params[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 62 %\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
